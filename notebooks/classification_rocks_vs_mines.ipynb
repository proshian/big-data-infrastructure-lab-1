{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","from typing import List, Tuple, Optional\n","import os\n","\n","import numpy as np\n","import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Looking into the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-25T22:10:02.243842Z","iopub.status.busy":"2023-04-25T22:10:02.243549Z","iopub.status.idle":"2023-04-25T22:10:02.286077Z","shell.execute_reply":"2023-04-25T22:10:02.284871Z","shell.execute_reply.started":"2023-04-25T22:10:02.243813Z"},"trusted":true},"outputs":[],"source":["data_filename = \"../data/sonar.all-data\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(data_filename, header=None)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.iloc[:, :-1].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.iloc[:,-1].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for label in ['R', 'M']:\n","    cur_label_only = df[df.iloc[:, -1] == label]\n","    row_index = 0\n","    plt.plot(cur_label_only.iloc[row_index, :-1])\n","    plt.title(f'example of {label} frequencies')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset preparation for the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_size = 0.2\n","random_state = 42\n","\n","X = df.iloc[:, :-1].values\n","y = df.iloc[:, -1].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=test_size, random_state=random_state, stratify=y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SonarDataset(torch.utils.data.Dataset):\n","    i2label = ['R', 'M']\n","    label2i = {label: i for i, label in enumerate(i2label)}\n","\n","    def __init__(self, X, labels):\n","        y_list = [self.label2i[label] for label in labels]\n","        self.y = torch.tensor(y_list, dtype = torch.float32).view(-1, 1) \n","        self.X = torch.tensor(X.values, dtype = torch.float32)\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","    \n","    def get_classes_distribution(self):\n","        return Counter(map(lambda i: self.i2label[int(i)], self.y.numpy().flatten()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_train_test_datasets(path_to_csv: str, test_size: float = 0.2, random_state: int = 42):\n","    df = pd.read_csv(path_to_csv, index_col = False, header = None)\n","    X = df.iloc[:, :-1]\n","    y = df.iloc[:, -1]\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size = test_size, random_state=random_state, stratify=y)\n","    train_dataset = SonarDataset(X_train, y_train)\n","    test_dataset = SonarDataset(X_test, y_test)\n","    return train_dataset, test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset, test_dataset = get_train_test_datasets(data_filename)\n","datasets = {'train': train_dataset, 'test': test_dataset}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for phase, dataset in datasets.items():\n","    print(f\"{phase} classes distribution: {dataset.get_classes_distribution()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataloaders = {\n","    # phase: torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n","    phase: torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n","    for phase, dataset in datasets.items()\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Train script"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_binary_classifier(model, criterion, optimizer,\n","                       num_epochs=3, phases = ['train', 'test'],\n","                       history = None, device = None, threshold = 0.5):\n","    \"\"\"\n","    At the moment history is supposed to be a dict with keys 'train' and 'test'\n","    and values being dicts with keys 'loss', 'accuracy', f_score and values being lists of floats.\n","    \"\"\"\n","\n","    if device is None:\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    with tqdm (range(num_epochs)) as pbar:\n","        for epoch in pbar:\n","            for phase in phases:\n","                if phase == 'train':\n","                    model.train()\n","                else:\n","                    model.eval()\n","\n","                running_loss = 0.0\n","                running_corrects = 0.0\n","                all_true_labels = []\n","                all_preds = []\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    for inputs, labels in dataloaders[phase]:\n","                        inputs = inputs.to(device)\n","                        labels = labels.to(device)\n","                        \n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                        if phase == 'train':\n","                            optimizer.zero_grad()\n","                            loss.backward()\n","                            optimizer.step()\n","\n","                        threshold = 0.5\n","                        preds = outputs > threshold\n","                        \n","                        running_loss += loss.item()\n","                        running_corrects += torch.sum(preds == labels.data)\n","\n","                        all_true_labels.extend(labels.tolist())\n","                        all_preds.extend(preds.tolist())\n","\n","                epoch_loss = running_loss / len(datasets[phase])\n","                epoch_acc = running_corrects.item() / len(datasets[phase])\n","                epoch_f_score = f1_score(all_true_labels, all_preds, average = 'macro')\n","                \n","                if history is not None:\n","                    history[phase][\"f_score\"].append(epoch_f_score)\n","                    history[phase][\"loss\"].append(epoch_loss)\n","                    history[phase][\"accuracy\"].append(epoch_acc)\n","\n","                # print(f\"{phase} loss: {epoch_loss:.4f}, f_score: {epoch_f_score:.4f}\")\n","                pbar.set_description(f\"{phase} loss: {epoch_loss:.4f}, f_score: {epoch_f_score:.4f}, accuracy: {epoch_acc}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pytorch model that applies precomputed mean and variance to the data\n","class NormalizedModel(torch.nn.Module):\n","    def __init__(self, model, mean, var):\n","        super().__init__()\n","        self.model = model\n","        self.mean = mean\n","        self.var = var\n","\n","    def forward(self, x):\n","        x = (x - self.mean) / self.var\n","        return self.model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = torch.nn.Sequential(\n","    torch.nn.Linear(60, 40),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(40, 1),\n","    torch.nn.Sigmoid()\n",")\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = torch.nn.BCELoss()\n","\n","history = {'train': {'loss': [], 'accuracy': [], 'f_score': []}, 'test': {'loss': [], 'accuracy': [], 'f_score': []}}\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_mean = torch.tensor(X_train.mean(), dtype=torch.float32).view(1, -1)\n","X_train_var = torch.tensor(X_train.var(), dtype=torch.float32).view(1, -1)\n","\n","model_normalized = NormalizedModel(\n","    torch.nn.Sequential(\n","        torch.nn.Linear(60, 40),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(40, 1),\n","        torch.nn.Sigmoid()\n","    ),\n","    X_train_mean,\n","    X_train_var)\n","\n","optimizer_normalized = torch.optim.Adam(model_normalized.parameters(), lr=0.001)\n","criterion_normalized = torch.nn.BCELoss()\n","\n","history_normalized = {'train': {'loss': [], 'accuracy': [], 'f_score': []}, 'test': {'loss': [], 'accuracy': [], 'f_score': []}}\n","\n","model_normalized"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_dropout = torch.nn.Sequential(\n","    torch.nn.Linear(60, 40),\n","    torch.nn.ReLU(),\n","    torch.nn.Dropout(0.5),\n","    torch.nn.Linear(40, 1),\n","    torch.nn.Sigmoid()\n",")\n","\n","optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=0.001)\n","criterion_dropout = torch.nn.BCELoss()\n","\n","history_dropout = {'train': {'loss': [], 'accuracy': [], 'f_score': []}, 'test': {'loss': [], 'accuracy': [], 'f_score': []}}\n","\n","model_dropout"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_binary_classifier(model_normalized, criterion_normalized, optimizer_normalized, num_epochs=60, history = history_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_binary_classifier(model, criterion, optimizer, num_epochs=60, history = history)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_binary_classifier(model_dropout, criterion_dropout, optimizer_dropout, num_epochs=60, history = history_dropout)"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import List, Tuple, Dict\n","import matplotlib.pyplot as plt\n","\n","\n","def get_phases_and_metric_names_from_hirtory_list(history_list: List[Dict[str, List[float]]]) -> Tuple[List[str], List[str]]:\n","    history1 = history_list[0]\n","    phases = list(history1.keys())\n","    metric_names = list(history1[phases[0]].keys())\n","    return phases, metric_names\n","\n","\n","def get_epoches_num_from_history(history: Dict[str, List[float]], phases: List[str], metric_names: List[str]) -> int:\n","    return len(history[phases[0]][metric_names[0]])\n","\n","\n","def plot_history(history_list: List[Dict[str, List[float]]],\n","                 history_names: List[str] = None,\n","                 omit_first_epoch: bool = False,\n","                 force_legend: bool = False,\n","                 img_name: str = None) -> None:\n","    \"\"\"\n","    Plots histories on same plot\n","\n","    history_list is a list of histories. A history is a dict with phase\n","    names as keys and a an inner dict as values. The inner dict has metric\n","    names as keys and list of metric values as values.\n","    Here is an example of a history:\n","        {\n","            'train': {\n","                'accuracy': [0.1, 0.2, 0.3, ...],\n","                'loss': [0.1, 0.2, 0.3, ...],\n","                'f1_score': [0.1, 0.2, 0.3, ...],\n","            },\n","            'test': {\n","                'accuracy': [0.1, 0.2, 0.3, ...],\n","                'loss': [0.1, 0.2, 0.3, ...],\n","                'f1_score': [0.1, 0.2, 0.3, ...],\n","            }\n","        }\n","    \n","    The resulting plot is a grid of suplots with (phases number) rows\n","    and (metric names number) columns. Each subplot contains metric values for all histories.\n","\n","    The legend is present if len(history_list) > 1 or force_legend is True. If history_names is not None,\n","    it is used as legend labels. Otherwise, history_list indexes are used as legend labels.\n","\n","    Args:\n","        history_list (List[dict[str, List[float]]]): list of histories.\n","        history_names (List[str], optional):\n","            list of history names. Defaults to None.\n","        omit_first_epoch (bool, optional):\n","            if True, first epoch will be omitted. Defaults to False.\n","        force_legend (bool, optional): if True, legend will be\n","            present even if len(history_list) == 1. Defaults to False.\n","        img_name (str, optional): if not None, the plot will be\n","            saved to img_name. Defaults to None.\n","    \"\"\"\n","\n","    assert len(history_list) > 0, \"history_list is empty\"\n","\n","    phases, metric_names = get_phases_and_metric_names_from_hirtory_list(history_list)\n","\n","    if history_names is None:\n","        history_list_indexes = range(len(history_list))\n","        history_names = history_list_indexes\n","    else:    \n","        assert len(history_list) == len(history_names), \"len(history_list) != len(history_names)\"\n","\n","    max_epochs_num = max([get_epoches_num_from_history(history, phases, metric_names) for history in history_list])\n","    \n","    fig, axs = plt.subplots(len(phases), len(metric_names))\n","\n","    fig.set_figheight(10)\n","    fig.set_figwidth(20)\n","\n","    start = 1 if omit_first_epoch else 0\n","\n","    for phase_index, phase in enumerate(phases):\n","        for metric_index, metric_name in enumerate(metric_names):\n","            ax = axs[phase_index][metric_index]\n","            ax.set_title(f\"{phase} {metric_name}\")\n","            ax.set_xticks(range(max_epochs_num - start))\n","            ax.set_xticklabels(range(start + 1, max_epochs_num + 1))\n","\n","            for history, name in zip(history_list, history_names):\n","                ax.plot(history[phase][metric_name][start:], label = name)\n","            \n","            if force_legend or len(history_list) > 1:\n","                ax.legend()\n","    \n","    if img_name is not None:\n","        plt.savefig(img_name)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_history([history, history_normalized, history_dropout], ['model', 'with normalized data', 'model_dropout'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index = 1\n","threshold = 0.5\n","model(datasets['test'][index][0]).item(), datasets['train'][index][1].item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
